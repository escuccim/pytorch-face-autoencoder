{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAEGAN_Implementation_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/escuccim/pytorch-face-autoencoder/blob/master/VAEGAN_Implementation_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KPgUFmQBlXj",
        "colab_type": "code",
        "outputId": "ffe1d182-4841-4cb9-a8fb-c3e1ca366dff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from matplotlib import pyplot as plt, colors\n",
        "%matplotlib inline\n",
        "import zipfile\n",
        "import os\n",
        "import matplotlib.animation as animation\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "from torch.optim import RMSprop,Adam,SGD\n",
        "from torch.optim.lr_scheduler import ExponentialLR,MultiStepLR\n",
        "import progressbar\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# upload checkpoint to GCS\n",
        "project_id = 'mammography-198911'\n",
        "bucket_name = 'gan-faces'\n",
        "\n",
        "!gcloud config set project {project_id}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH8YnifpBxQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"celeba-dataset.zip\"):\n",
        "#   !kaggle datasets download -d jessicali9530/celeba-dataset\n",
        "  !gsutil cp gs://{bucket_name}/celeba-dataset.zip ./celeba-dataset.zip\n",
        "  zip_ref = zipfile.ZipFile('celeba-dataset.zip', 'r')\n",
        "  zip_ref.extractall('data')\n",
        "  zip_ref.close()\n",
        "\n",
        "  zip_ref = zipfile.ZipFile('data/img_align_celeba.zip', 'r')\n",
        "  zip_ref.extractall('data/images')\n",
        "  zip_ref.close()\n",
        "\n",
        "# if not os.path.exists(\"wiki_images_good.zip\"):\n",
        "#   !gsutil cp gs://{bucket_name}/wiki_images_good.zip ./ \n",
        "#   zip_ref = zipfile.ZipFile('wiki_images_good.zip', 'r')\n",
        "#   zip_ref.extractall('data/wiki_images')\n",
        "#   zip_ref.close()  \n",
        "\n",
        "# if not os.path.exists(\"celeb_a_good.zip\"):\n",
        "# #   !kaggle datasets download -d jessicali9530/celeba-dataset\n",
        "#   !gsutil cp gs://{bucket_name}/celeb_a_good.zip ./celeb_a_good.zip\n",
        "#   zip_ref = zipfile.ZipFile('celeb_a_good.zip', 'r')\n",
        "#   zip_ref.extractall('data/images/celeb_a')\n",
        "#   zip_ref.close()\n",
        "\n",
        "# if not os.path.exists(\"Training_Pictures.zip\"):\n",
        "#   !wget https://s3.eu-west-3.amazonaws.com/deep.skoo.ch/Training_Pictures.zip\n",
        "#   zip_ref = zipfile.ZipFile('Training_Pictures.zip', 'r')\n",
        "#   zip_ref.extractall('data/images/training')\n",
        "#   zip_ref.close()\n",
        "  \n",
        "# if not os.path.exists(\"imdb_crop_good.zip\"):  \n",
        "#   !gsutil cp gs://{bucket_name}/imdb_crop_good.zip ./imdb_crop_good.zip\n",
        "#   zip_ref = zipfile.ZipFile('imdb_crop_good.zip', 'r')\n",
        "#   zip_ref.extractall('data/images/imdb')\n",
        "#   zip_ref.close()\n",
        "\n",
        "# if not os.path.exists(\"crop_part1_good.zip\"):\n",
        "#   !gsutil cp gs://{bucket_name}/crop_part1_good.zip ./crop_part1_good.zip\n",
        "#   zip_ref = zipfile.ZipFile('crop_part1_good.zip', 'r')\n",
        "#   zip_ref.extractall('data/images/crop_parta')\n",
        "#   zip_ref.close()   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i236_bgmB7uT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"wiki_images_good.zip\"):\n",
        "  !gsutil cp gs://{bucket_name}/wiki_images_good.zip ./wiki_images_good.zip\n",
        "  zip_ref = zipfile.ZipFile('wiki_images_good.zip', 'r')\n",
        "  zip_ref.extractall('data/test_images')\n",
        "  zip_ref.close()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBihixS9B8_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoder block (used in encoder and discriminator)\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, channel_in, channel_out):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        # convolution to halve the dimensions\n",
        "        self.conv = nn.Conv2d(in_channels=channel_in, out_channels=channel_out, kernel_size=5, padding=2, stride=2,\n",
        "                              bias=False)\n",
        "        self.bn = nn.BatchNorm2d(num_features=channel_out, momentum=0.9)\n",
        "\n",
        "    def forward(self, ten, out=False,t = False):\n",
        "        # here we want to be able to take an intermediate output for reconstruction error\n",
        "        if out:\n",
        "            ten = self.conv(ten)\n",
        "            ten_out = ten\n",
        "            ten = self.bn(ten)\n",
        "            ten = F.relu(ten, False)\n",
        "            return ten, ten_out\n",
        "        else:\n",
        "            ten = self.conv(ten)\n",
        "            ten = self.bn(ten)\n",
        "            ten = F.relu(ten, True)\n",
        "            return ten\n",
        "\n",
        "\n",
        "# decoder block (used in the decoder)\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, channel_in, channel_out):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        # transpose convolution to double the dimensions\n",
        "        self.conv = nn.ConvTranspose2d(channel_in, channel_out, kernel_size=5, padding=2, stride=2, output_padding=1,\n",
        "                                       bias=False)\n",
        "        self.bn = nn.BatchNorm2d(channel_out, momentum=0.9)\n",
        "\n",
        "    def forward(self, ten):\n",
        "        ten = self.conv(ten)\n",
        "        ten = self.bn(ten)\n",
        "        ten = F.relu(ten, True)\n",
        "        return ten\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, channel_in=3, z_size=128):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.size = channel_in\n",
        "        layers_list = []\n",
        "        # the first time 3->64, for every other double the channel size\n",
        "        for i in range(4):\n",
        "            if i == 0:\n",
        "                layers_list.append(EncoderBlock(channel_in=self.size, channel_out=64))\n",
        "                self.size = 64\n",
        "            else:\n",
        "                layers_list.append(EncoderBlock(channel_in=self.size, channel_out=self.size * 2))\n",
        "                self.size *= 2\n",
        "\n",
        "        # final shape Bx256x8x8\n",
        "        self.conv = nn.Sequential(*layers_list)\n",
        "        self.fc = nn.Sequential(nn.Linear(in_features=8 * 8 * self.size, out_features=1024, bias=False),\n",
        "                                nn.BatchNorm1d(num_features=1024,momentum=0.9),\n",
        "                                nn.ReLU(True))\n",
        "        # two linear to get the mu vector and the diagonal of the log_variance\n",
        "        self.l_mu = nn.Linear(in_features=1024, out_features=z_size)\n",
        "        self.l_var = nn.Linear(in_features=1024, out_features=z_size)\n",
        "\n",
        "    def forward(self, ten):\n",
        "        ten = self.conv(ten)\n",
        "        ten = ten.view(len(ten), -1)\n",
        "        ten = self.fc(ten)\n",
        "        mu = self.l_mu(ten)\n",
        "        logvar = self.l_var(ten)\n",
        "        return mu, logvar\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return super(Encoder, self).__call__(*args, **kwargs)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, z_size, size):\n",
        "        super(Decoder, self).__init__()\n",
        "        # start from B*z_size\n",
        "        self.fc = nn.Sequential(nn.Linear(in_features=z_size, out_features=8 * 8 * size, bias=False),\n",
        "                                nn.BatchNorm1d(num_features=8 * 8 * size,momentum=0.9),\n",
        "                                nn.ReLU(True))\n",
        "        self.size = size\n",
        "        layers_list = []\n",
        "        layers_list.append(DecoderBlock(channel_in=self.size, channel_out=self.size))\n",
        "        layers_list.append(DecoderBlock(channel_in=self.size, channel_out=self.size//2))\n",
        "        self.size = self.size//2\n",
        "        layers_list.append(DecoderBlock(channel_in=self.size, channel_out=self.size//2))\n",
        "        self.size = self.size//2\n",
        "        layers_list.append(DecoderBlock(channel_in=self.size, channel_out=self.size//2))\n",
        "        self.size = self.size//2\n",
        "        # final conv to get 3 channels and tanh layer\n",
        "        layers_list.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels=self.size, out_channels=3, kernel_size=5, stride=1, padding=2),\n",
        "            nn.Tanh()\n",
        "        ))\n",
        "\n",
        "        self.conv = nn.Sequential(*layers_list)\n",
        "\n",
        "    def forward(self, ten):\n",
        "\n",
        "        ten = self.fc(ten)\n",
        "        ten = ten.view(len(ten), -1, 8, 8)\n",
        "        ten = self.conv(ten)\n",
        "        return ten\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return super(Decoder, self).__call__(*args, **kwargs)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channel_in=3,recon_level=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.size = channel_in\n",
        "        self.recon_levl = recon_level\n",
        "        # module list because we need need to extract an intermediate output\n",
        "        self.conv = nn.ModuleList()\n",
        "        self.conv.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        self.size = 32\n",
        "        self.conv.append(EncoderBlock(channel_in=self.size, channel_out=self.size*2))\n",
        "        self.size *= 2\n",
        "        self.conv.append(EncoderBlock(channel_in=self.size, channel_out=self.size*2))\n",
        "        self.size *= 2\n",
        "        self.conv.append(EncoderBlock(channel_in=self.size, channel_out=self.size*2))\n",
        "        self.size *= 2\n",
        "        self.conv.append(EncoderBlock(channel_in=self.size, channel_out=self.size*2))\n",
        "        self.size *= 2\n",
        "        # final fc to get the score (real or fake)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_features=8 * 8 * self.size, out_features=512, bias=False),\n",
        "            nn.BatchNorm1d(num_features=512,momentum=0.9),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=512, out_features=1),\n",
        "\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, ten,ten_original,ten_sampled):\n",
        "\n",
        "        ten = torch.cat((ten, ten_original,ten_sampled), 0)\n",
        "\n",
        "        for i, lay in enumerate(self.conv):\n",
        "            # we take the 9th layer as one of the outputs\n",
        "            if i == self.recon_levl:\n",
        "                ten, layer_ten = lay(ten, True)\n",
        "                # we need the layer representations just for the original and reconstructed,\n",
        "                # flatten, because it's a convolutional shape\n",
        "                layer_ten = layer_ten.view(len(layer_ten), -1)\n",
        "            else:\n",
        "                ten = lay(ten)\n",
        "\n",
        "        ten = ten.view(len(ten), -1)\n",
        "        ten = self.fc(ten)\n",
        "        return layer_ten, self.sigmoid(ten)\n",
        "\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return super(Discriminator, self).__call__(*args, **kwargs)\n",
        "\n",
        "\n",
        "class VaeGan(nn.Module):\n",
        "    def __init__(self,z_size=128,recon_level=3):\n",
        "        super(VaeGan, self).__init__()\n",
        "        # latent space size\n",
        "        self.z_size = z_size\n",
        "        self.encoder = Encoder(z_size=self.z_size)\n",
        "        self.decoder = Decoder(z_size=self.z_size, size=self.encoder.size)\n",
        "        self.discriminator = Discriminator(channel_in=3,recon_level=recon_level)\n",
        "        # self-defined function to init the parameters\n",
        "        self.init_parameters()\n",
        "\n",
        "    def init_parameters(self):\n",
        "        # just explore the network, find every weight and bias matrix and fill it\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
        "                if hasattr(m, \"weight\") and m.weight is not None and m.weight.requires_grad:\n",
        "                    #init as original implementation\n",
        "                    scale = 1.0/numpy.sqrt(numpy.prod(m.weight.shape[1:]))\n",
        "                    scale /=numpy.sqrt(3)\n",
        "                    #nn.init.xavier_normal(m.weight,1)\n",
        "                    #nn.init.constant(m.weight,0.005)\n",
        "                    nn.init.uniform(m.weight,-scale,scale)\n",
        "                if hasattr(m, \"bias\") and m.bias is not None and m.bias.requires_grad:\n",
        "                    nn.init.constant(m.bias, 0.0)\n",
        "\n",
        "    def forward(self, ten, gen_size=10):\n",
        "        if self.training:\n",
        "            # save the original images\n",
        "            ten_original = ten\n",
        "            # encode\n",
        "            mus, log_variances = self.encoder(ten)\n",
        "            # we need the true variances, not the log one\n",
        "            variances = torch.exp(log_variances * 0.5)\n",
        "            # sample from a gaussian\n",
        "\n",
        "            ten_from_normal = Variable(torch.randn(len(ten), self.z_size).cuda(), requires_grad=True)\n",
        "            # shift and scale using the means and variances\n",
        "\n",
        "            ten = ten_from_normal * variances + mus\n",
        "            # decode the tensor\n",
        "            ten = self.decoder(ten)\n",
        "            ten_from_normal = Variable(torch.randn(len(ten), self.z_size).cuda(), requires_grad=True)\n",
        "            ten_from_normal = self.decoder(ten_from_normal)\n",
        "            #discriminator\n",
        "            ten_layer,ten_class = self.discriminator(ten,ten_original,ten_from_normal)\n",
        "\n",
        "            return ten, ten_class, ten_layer, mus, log_variances\n",
        "\n",
        "        else:\n",
        "            if ten is None:\n",
        "                # just sample and decode\n",
        "\n",
        "                ten = Variable(torch.randn(gen_size, self.z_size).cuda(), requires_grad=False)\n",
        "                ten = self.decoder(ten)\n",
        "            else:\n",
        "                mus, log_variances = self.encoder(ten)\n",
        "                # we need the true variances, not the log one\n",
        "                variances = torch.exp(log_variances * 0.5)\n",
        "                # sample from a gaussian\n",
        "\n",
        "                ten_from_normal = Variable(torch.randn(len(ten), self.z_size).cuda(), requires_grad=False)\n",
        "                # shift and scale using the means and variances\n",
        "                ten = ten_from_normal * variances + mus\n",
        "                # decode the tensor\n",
        "                ten = self.decoder(ten)\n",
        "            return ten\n",
        "\n",
        "\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return super(VaeGan, self).__call__(*args, **kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def loss(ten_original, ten_predicted, layer_original, layer_predicted,layer_sampled, labels_original,\n",
        "             labels_predicted,labels_sampled, mus, variances):\n",
        "        \"\"\"\n",
        "        :param ten_original: original images\n",
        "        :param ten_predicted:  predicted images (output of the decoder)\n",
        "        :param layer_original:  intermediate layer for original (intermediate output of the discriminator)\n",
        "        :param layer_predicted: intermediate layer for reconstructed (intermediate output of the discriminator)\n",
        "        :param labels_original: labels for original (output of the discriminator)\n",
        "        :param labels_predicted: labels for reconstructed (output of the discriminator)\n",
        "        :param labels_sampled: labels for sampled from gaussian (0,1) (output of the discriminator)\n",
        "        :param mus: tensor of means\n",
        "        :param variances: tensor of diagonals of log_variances\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        # reconstruction error, not used for the loss but useful to evaluate quality\n",
        "        nle = 0.5*(ten_original.view(len(ten_original), -1) - ten_predicted.view(len(ten_predicted), -1)) ** 2\n",
        "        # kl-divergence\n",
        "        kl = -0.5 * torch.sum(-variances.exp() - torch.pow(mus,2) + variances + 1, 1)\n",
        "        # mse between intermediate layers for both\n",
        "        mse_1 = torch.sum(0.5*(layer_original - layer_predicted) ** 2, 1)\n",
        "        mse_2 = torch.sum(0.5*(layer_original - layer_sampled) ** 2, 1)\n",
        "        # bce for decoder and discriminator for original,sampled and reconstructed\n",
        "        # the only excluded is the bce_gen_original\n",
        "\n",
        "        bce_dis_original = -torch.log(labels_original + 1e-3)\n",
        "        bce_dis_sampled = -torch.log(1 - labels_sampled + 1e-3)\n",
        "        bce_dis_recon = -torch.log(1 - labels_predicted+ 1e-3)\n",
        "\n",
        "        #bce_gen_original = -torch.log(1-labels_original + 1e-3)\n",
        "        bce_gen_sampled = -torch.log(labels_sampled + 1e-3)\n",
        "        bce_gen_recon = -torch.log(labels_predicted+ 1e-3)\n",
        "        '''\n",
        "        \n",
        "        bce_gen_predicted = nn.BCEWithLogitsLoss(size_average=False)(labels_predicted,\n",
        "                                         Variable(torch.ones_like(labels_predicted.data).cuda(), requires_grad=False))\n",
        "        bce_gen_sampled = nn.BCEWithLogitsLoss(size_average=False)(labels_sampled,\n",
        "                                       Variable(torch.ones_like(labels_sampled.data).cuda(), requires_grad=False))\n",
        "        bce_dis_original = nn.BCEWithLogitsLoss(size_average=False)(labels_original,\n",
        "                                        Variable(torch.ones_like(labels_original.data).cuda(), requires_grad=False))\n",
        "        bce_dis_predicted = nn.BCEWithLogitsLoss(size_average=False)(labels_predicted,\n",
        "                                         Variable(torch.zeros_like(labels_predicted.data).cuda(), requires_grad=False))\n",
        "        bce_dis_sampled = nn.BCEWithLogitsLoss(size_average=False)(labels_sampled,\n",
        "                                       Variable(torch.zeros_like(labels_sampled.data).cuda(), requires_grad=False))\n",
        "        '''\n",
        "        return nle, kl, mse_1,mse_2,\\\n",
        "               bce_dis_original, bce_dis_sampled,bce_dis_recon,bce_gen_sampled,bce_gen_recon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnQmRuirCBWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RollingMeasure(object):\n",
        "    def __init__(self):\n",
        "        self.measure = 0.0\n",
        "        self.iter = 0\n",
        "\n",
        "    def __call__(self, measure):\n",
        "        # passo nuovo valore e ottengo average\n",
        "        # se first call inizializzo\n",
        "        if self.iter == 0:\n",
        "            self.measure = measure\n",
        "        else:\n",
        "            self.measure = (1.0 / self.iter * measure) + (1 - 1.0 / self.iter) * self.measure\n",
        "        self.iter += 1\n",
        "        return self.measure"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47mGUdaYCEwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "from skimage import filters,transform\n",
        "numpy.random.seed(5)\n",
        "\n",
        "def _resize(img):\n",
        "    rescale_size = 64\n",
        "    bbox = (40, 218 - 30, 15, 178 - 15)\n",
        "    img = img[bbox[0]:bbox[1], bbox[2]:bbox[3]]\n",
        "    # Smooth image before resize to avoid moire patterns\n",
        "    scale = img.shape[0] / float(rescale_size)\n",
        "    sigma = numpy.sqrt(scale) / 2.0\n",
        "    img = filters.gaussian(img, sigma=sigma, multichannel=True)\n",
        "    img = transform.resize(img, (rescale_size, rescale_size, 3), order=3,mode=\"constant\")\n",
        "    img = (img*255).astype(numpy.uint8)\n",
        "    return img\n",
        "\n",
        "class CELEBA(Dataset):\n",
        "    \"\"\"\n",
        "    loader for the CELEB-A dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_folder):\n",
        "        #len is the number of files\n",
        "        self.len = len(os.listdir(data_folder))\n",
        "        #list of file names\n",
        "        self.data_names = [os.path.join(data_folder, name) for name in sorted(os.listdir(data_folder))]\n",
        "        #data_all\n",
        "        #if \"train\" in data_folder:\n",
        "        #    self.data = numpy.load(\"/home/lapis/Desktop/full_train.npy\")\n",
        "        #else:\n",
        "        #    self.data = numpy.load(\"/home/lapis/Desktop/full_test.npy\")\n",
        "\n",
        "        self.len = len(self.data_names)\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        \"\"\"\n",
        "        :param item: image index between 0-(len-1)\n",
        "        :return: image\n",
        "        \"\"\"\n",
        "        #load image,crop 128x128,resize,transpose(to channel first),scale (so we can use tanh)\n",
        "        data = cv2.cvtColor(cv2.imread(self.data_names[item]), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        data = _resize(data)\n",
        "\n",
        "        # CHANNEL FIRST\n",
        "        data = data.transpose(2, 0, 1)\n",
        "        # TANH\n",
        "        data = data.astype(\"float32\") / 127.5 - 1.0\n",
        "\n",
        "        return (data.copy(),data.copy())\n",
        "\n",
        "\n",
        "class CELEBA_SLURM(Dataset):\n",
        "    \"\"\"\n",
        "    loader for the CELEB-A dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_folder):\n",
        "        #open the file\n",
        "        self.file = open(os.path.join(data_folder,\"images/img_align_celeba\"),\"rb\")\n",
        "        #get len\n",
        "        self.len = int(os.path.getsize(os.path.join(data_folder,\"images\"))/(64*64*3))\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        \"\"\"\n",
        "        :param item: image index between 0-(len-1)\n",
        "        :return: image\n",
        "        \"\"\"\n",
        "        offset = item*3*64*64\n",
        "        self.file.seek(offset)\n",
        "        data = numpy.fromfile(self.file, dtype=numpy.uint8, count=(3 * 64 * 64))\n",
        "        data = numpy.reshape(data, newshape=(3, 64, 64))\n",
        "        data = data.astype(\"float32\") / 127.5 - 1.0\n",
        "        return (data.copy(),data.copy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJglm8xaCb4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_folder = \"data/images\"\n",
        "# dataloader = torch.utils.data.DataLoader(CELEBA(train_folder), batch_size=64,\n",
        "#                                                  shuffle=True, num_workers=4)\n",
        "\n",
        "# test_folder = \"data/test_images\"\n",
        "# dataloader_test = torch.utils.data.DataLoader(CELEBA(test_folder), batch_size=100,\n",
        "#                                                       shuffle=False, num_workers=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyJPZnmlEKaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = BATCH_SIZE = 48\n",
        "data_path = \"data/images\"\n",
        "image_size = (128,128)\n",
        "\n",
        "transform = torchvision.transforms.Compose(\n",
        "    [torchvision.transforms.RandomHorizontalFlip(p=0.5), \n",
        "      torchvision.transforms.RandomApply([\n",
        "        torchvision.transforms.RandomAffine(degrees=2, translate=(0.05,0.05), scale=(0.95,1.05), shear=2, resample=False, fillcolor=0),        \n",
        "      ], 0.7),\n",
        "      torchvision.transforms.RandomResizedCrop(image_size, scale=(0.9, 1.10)),\n",
        "      torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "transform2 = torchvision.transforms.Compose(\n",
        "    [torchvision.transforms.RandomResizedCrop(image_size, scale=(0.95, 1.05)),\n",
        "      torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(\n",
        "        root=data_path,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.ImageFolder(\n",
        "        root=\"data/test_images\",\n",
        "        transform=transform2\n",
        "    )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=192,\n",
        "    num_workers=2,\n",
        "    shuffle=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5SWp7Y9COTT",
        "colab_type": "code",
        "outputId": "b642f94a-5b8b-4d4d-eae9-12c2195a9795",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "z_size = 128\n",
        "recon_level = 4\n",
        "decay_lr = 0.75\n",
        "lambda_mse = 1e-6\n",
        "lr = 3e-4\n",
        "decay_margin = 1\n",
        "decay_equilibrium = 1\n",
        "\n",
        "net = VaeGan(z_size=z_size,recon_level=recon_level).cuda()\n",
        "\n",
        "margin = 0.35\n",
        "equilibrium = 0.68\n",
        "#mse_lambda = 1.0\n",
        "# OPTIM-LOSS\n",
        "# an optimizer for each of the sub-networks, so we can selectively backprop\n",
        "#optimizer_encoder = Adam(params=net.encoder.parameters(),lr = lr,betas=(0.9,0.999))\n",
        "optimizer_encoder = RMSprop(params=net.encoder.parameters(),lr=lr,alpha=0.9,eps=1e-8,weight_decay=0,momentum=0,centered=False)\n",
        "#lr_encoder = MultiStepLR(optimizer_encoder,milestones=[2],gamma=1)\n",
        "lr_encoder = ExponentialLR(optimizer_encoder, gamma=decay_lr)\n",
        "#optimizer_decoder = Adam(params=net.decoder.parameters(),lr = lr,betas=(0.9,0.999))\n",
        "optimizer_decoder = RMSprop(params=net.decoder.parameters(),lr=lr,alpha=0.9,eps=1e-8,weight_decay=0,momentum=0,centered=False)\n",
        "lr_decoder = ExponentialLR(optimizer_decoder, gamma=decay_lr)\n",
        "#lr_decoder = MultiStepLR(optimizer_decoder,milestones=[2],gamma=1)\n",
        "#optimizer_discriminator = Adam(params=net.discriminator.parameters(),lr = lr,betas=(0.9,0.999))\n",
        "optimizer_discriminator = RMSprop(params=net.discriminator.parameters(),lr=lr,alpha=0.9,eps=1e-8,weight_decay=0,momentum=0,centered=False)\n",
        "lr_discriminator = ExponentialLR(optimizer_discriminator, gamma=decay_lr)\n",
        "#lr_discriminator = MultiStepLR(optimizer_discriminator,milestones=[2],gamma=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:183: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:185: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6MgBRMKDShR",
        "colab_type": "code",
        "outputId": "cfad59fc-8644-45ed-b7f2-7e49227295d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "decay_mse = 1\n",
        "decay_equilibrium = 1\n",
        "\n",
        "n_epochs = 1\n",
        "for i in range(n_epochs):\n",
        "    # reset rolling average\n",
        "    loss_nle_mean = RollingMeasure()\n",
        "    loss_encoder_mean = RollingMeasure()\n",
        "    loss_decoder_mean = RollingMeasure()\n",
        "    loss_discriminator_mean = RollingMeasure()\n",
        "    loss_reconstruction_layer_mean = RollingMeasure()\n",
        "    loss_kld_mean = RollingMeasure()\n",
        "    gan_gen_eq_mean = RollingMeasure()\n",
        "    gan_dis_eq_mean = RollingMeasure()\n",
        "    #print(\"LR:{}\".format(lr_encoder.get_lr()))\n",
        "\n",
        "    # for each batch\n",
        "    for j, (data_batch,target_batch) in enumerate(train_loader):\n",
        "        # set to train mode\n",
        "        train_batch = len(data_batch)\n",
        "        net.train()\n",
        "        # target and input are the same images\n",
        "        data_target = Variable(target_batch, requires_grad=False).float().cuda()\n",
        "        data_in = Variable(data_batch, requires_grad=False).float().cuda()\n",
        "\n",
        "\n",
        "        # get output\n",
        "        out, out_labels, out_layer, mus, variances = net(data_in)\n",
        "        # split so we can get the different parts\n",
        "        out_layer_predicted = out_layer[:train_batch]\n",
        "        out_layer_original = out_layer[train_batch:-train_batch]\n",
        "        out_layer_sampled = out_layer[-train_batch:]\n",
        "        #labels\n",
        "        out_labels_predicted = out_labels[:train_batch]\n",
        "        out_labels_original = out_labels[train_batch:-train_batch]\n",
        "        out_labels_sampled = out_labels[-train_batch:]\n",
        "        # loss, nothing special here\n",
        "        nle_value, kl_value, mse_value_1,mse_value_2, bce_dis_original_value, bce_dis_sampled_value, \\\n",
        "        bce_dis_predicted_value,bce_gen_sampled_value,bce_gen_predicted_value= VaeGan.loss(data_target, out, out_layer_original,\n",
        "                                                                      out_layer_predicted,out_layer_sampled, out_labels_original,\n",
        "                                                                      out_labels_predicted,out_labels_sampled, mus,\n",
        "                                                                      variances)\n",
        "        # THIS IS THE MOST IMPORTANT PART OF THE CODE\n",
        "        loss_encoder = torch.sum(kl_value)+torch.sum(mse_value_1)+torch.sum(mse_value_2)\n",
        "        loss_discriminator = torch.sum(bce_dis_original_value) + torch.sum(bce_dis_sampled_value)+ torch.sum(bce_dis_predicted_value)\n",
        "        loss_decoder = torch.sum(bce_gen_sampled_value) + torch.sum(bce_gen_predicted_value)\n",
        "        loss_decoder = torch.sum(lambda_mse/2 * mse_value_1)+ torch.sum(lambda_mse/2 * mse_value_2) + (1.0 - lambda_mse) * loss_decoder\n",
        "\n",
        "        # register mean values of the losses for logging\n",
        "        loss_nle_mean(torch.mean(nle_value).data.cpu().item())\n",
        "        loss_discriminator_mean((torch.mean(bce_dis_original_value) + torch.mean(bce_dis_sampled_value)).data.cpu().item())\n",
        "        loss_decoder_mean((torch.mean(lambda_mse * mse_value_1/2)+torch.mean(lambda_mse * mse_value_2/2) + (1 - lambda_mse) * (torch.mean(bce_gen_predicted_value) + torch.mean(bce_gen_sampled_value))).data.cpu().item())\n",
        "\n",
        "        loss_encoder_mean((torch.mean(kl_value) + torch.mean(mse_value_1)+ torch.mean(mse_value_2)).data.cpu().item())\n",
        "        loss_reconstruction_layer_mean((torch.mean(mse_value_1)+torch.mean(mse_value_2)).data.cpu().item())\n",
        "        loss_kld_mean(torch.mean(kl_value).data.cpu().item())\n",
        "        # selectively disable the decoder of the discriminator if they are unbalanced\n",
        "        train_dis = True\n",
        "        train_dec = True\n",
        "        if torch.mean(bce_dis_original_value).item() < equilibrium-margin or torch.mean(bce_dis_sampled_value).item() < equilibrium-margin:\n",
        "            train_dis = False\n",
        "        if torch.mean(bce_dis_original_value).item() > equilibrium+margin or torch.mean(bce_dis_sampled_value).item() > equilibrium+margin:\n",
        "            train_dec = False\n",
        "        if train_dec is False and train_dis is False:\n",
        "            train_dis = True\n",
        "            train_dec = True\n",
        "\n",
        "        #aggiungo log\n",
        "        if train_dis:\n",
        "            gan_dis_eq_mean(1.0)\n",
        "        else:\n",
        "            gan_dis_eq_mean(0.0)\n",
        "\n",
        "        if train_dec:\n",
        "            gan_gen_eq_mean(1.0)\n",
        "        else:\n",
        "            gan_gen_eq_mean(0.0)\n",
        "\n",
        "        # BACKPROP\n",
        "        # clean grads\n",
        "        net.zero_grad()\n",
        "        # encoder\n",
        "        loss_encoder.backward(retain_graph=True)\n",
        "        # someone likes to clamp the grad here\n",
        "        #[p.grad.data.clamp_(-1,1) for p in net.encoder.parameters()]\n",
        "        # update parameters\n",
        "        optimizer_encoder.step()\n",
        "        # clean others, so they are not afflicted by encoder loss\n",
        "        net.zero_grad()\n",
        "        #decoder\n",
        "        if train_dec:\n",
        "            loss_decoder.backward(retain_graph=True)\n",
        "            #[p.grad.data.clamp_(-1,1) for p in net.decoder.parameters()]\n",
        "            optimizer_decoder.step()\n",
        "            #clean the discriminator\n",
        "            net.discriminator.zero_grad()\n",
        "        #discriminator\n",
        "        if train_dis:\n",
        "            loss_discriminator.backward()\n",
        "            #[p.grad.data.clamp_(-1,1) for p in net.discriminator.parameters()]\n",
        "            optimizer_discriminator.step()\n",
        "        \n",
        "        if j % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_Dec: %.4f\\tLoss_Enc: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f'\n",
        "                  % (i, n_epochs, j, len(train_loader),\n",
        "                      loss_discriminator_mean.measure,  loss_decoder_mean.measure, loss_encoder_mean.measure, bce_dis_original_value.mean().item(), bce_dis_sampled_value.mean().item()))\n",
        "        if j % 500 == 0 and j > 0:\n",
        "            with torch.no_grad():\n",
        "              out = (out * 0.5) + 0.5\n",
        "              plt.imshow(np.transpose(vutils.make_grid(out[:4].detach().cpu(), nrow=2, padding=2, normalize=True),(1,2,0)))\n",
        "              plt.show()\n",
        "\n",
        "    torch.save(net.state_dict(), \"net2.pt\")\n",
        "\n",
        "    lr_encoder.step()\n",
        "    lr_decoder.step()\n",
        "    lr_discriminator.step()\n",
        "    margin *=decay_margin\n",
        "    equilibrium *=decay_equilibrium\n",
        "    #margin non puo essere piu alto di equilibrium\n",
        "    if margin > equilibrium:\n",
        "        equilibrium = margin\n",
        "    lambda_mse *=decay_mse\n",
        "    if lambda_mse > 1:\n",
        "        lambda_mse=1\n",
        "    \n",
        "    !gsutil cp ./net2.pt gs://{bucket_name}/net2.pt \n",
        "\n",
        "    # plot some sample images\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(8,8))\n",
        "    ax[0].imshow(np.transpose(vutils.make_grid(out.detach().cpu()[:16], nrow=4, padding=2, normalize=True),(1,2,0)))\n",
        "    ax[1].imshow(np.transpose(vutils.make_grid(data_batch.detach().cpu()[:16], nrow=4, padding=2, normalize=True),(1,2,0)))\n",
        "    plt.show()    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0/1][0/4221]\tLoss_D: 1.1326\tLoss_Dec: 3.2825\tLoss_Enc: 400996.8125\tD(x): 0.8796\tD(G(z)): 0.2529\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mht72d61pFU",
        "colab_type": "code",
        "outputId": "d329aef3-30b4-47ed-9104-3780f553c910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "torch.save(net.state_dict(), \"net2.pt\")\n",
        "!gsutil cp ./net2.pt gs://{bucket_name}/net2.pt "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://./net2.pt [Content-Type=application/octet-stream]...\n",
            "/ [0 files][    0.0 B/284.1 MiB]                                                \r==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\\n",
            "Operation completed over 1 objects/284.1 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUsiu8GdB-ga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(8,8))\n",
        "ax[0].imshow(np.transpose(vutils.make_grid(out.detach().cpu()[:16], nrow=4, padding=2, normalize=True),(1,2,0)))\n",
        "ax[1].imshow(np.transpose(vutils.make_grid(data_batch.detach().cpu()[:16], nrow=4, padding=2, normalize=True),(1,2,0)))\n",
        "plt.show()    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn_9FEhUB0Za",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}